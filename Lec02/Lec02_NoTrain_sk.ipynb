{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec02_NoTrain_sk.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenko99/Standalone_DDL/blob/master/Lec02/Lec02_NoTrain_sk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0hS2lAY3gsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "iris_data = iris.data\n",
        "iris_labels = iris.target\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(iris_data)\n",
        "df2 = pd.DataFrame(iris_labels)\n",
        "df = pd.concat([df, df2], axis=1, sort=False)\n",
        "df.columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Classes']\n",
        "\n",
        "np.random.seed(7)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "iris_data = df.iloc[:, :4].values\n",
        "iris_labels = df.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7L0YB8yMEjX",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Set train_x and train_y for training data\n",
        "\n",
        "Set test_x and test_y for testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDhBkSVt3nSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_train_data = 135\n",
        "\n",
        "train_x = iris_data[ : n_train_data]\n",
        "train_y = iris_labels[ : n_train_data]\n",
        "test_x = iris_data[n_train_data : ]\n",
        "test_y = iris_labels[n_train_data : ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1KeNQMMMMU-",
        "colab_type": "text"
      },
      "source": [
        "# Score Function\n",
        "\n",
        "$ f(X) = WX + B$\n",
        "\n",
        "$W.shape = (3,4)$\n",
        "\n",
        "$X.shape = (4, )$\n",
        "\n",
        "$B.shape = (3, )$\n",
        "\n",
        "$f(X).shape = (3, )$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-w5gDbzgsjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_function(X, W, B):\n",
        "    \"\"\"\n",
        "    \n",
        "    W는 (3,4) 차원 행렬이고, X는 (4,), B는 (3,)의 차원을 갖습니다.\n",
        "    \n",
        "    우리가 원하는 score function을 구하기 위해서는 W와 X를 내적한 뒤, B와 element wise하게 덧셈을 해야 할 것입니다.\n",
        "    \n",
        "    따라서 이러한 형태가 되어야 할 것입니다.\n",
        "    \n",
        "    return W 내적 X + B\n",
        "    \n",
        "    Hint : Lec00 Colab&Numpy tutorial에 행렬곱(W*X가 아닙니다)과 element wise한 행렬간의 덧셈이 무엇인지 나와있습니다.\n",
        "    \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_OaL9wJPm8p",
        "colab_type": "text"
      },
      "source": [
        "#SVM Loss \n",
        "\n",
        "##$L_{i} = \\underset{j\\neq y_i}\\sum max(0 \\ ,\\ s_j - s_{y_i} + \\triangle )$\n",
        "\n",
        "##$L = {1\\over N}\\underset{i=1}{\\overset{N}\\sum} L_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP__GadeN82b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svm_loss(scores, y_i):\n",
        "    \"\"\"\n",
        "    \n",
        "    이 svm_loss function은 위의 L_i를 구하는 함수입니다.\n",
        "    \n",
        "    scores는 score function을 통해 구해진 (3,)차원의 벡터(배열)입니다.\n",
        "    \n",
        "    즉, scores[0]는 setosa에 대한 score를 의미하는 것입니다.\n",
        "    \n",
        "    y_i는 L_i를 구할 때 나오는 y_i와 같습니다.\n",
        "    \n",
        "    즉, 정답을 가리킵니다.\n",
        "    \n",
        "    따라서 scores[y_i]는 정답 클래스에 대한 score를 의미할 것입니다.\n",
        "    \n",
        "    L_i 를 구할 때, margin은 1로 계산하시길 바랍니다.\n",
        "    \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epij5xAdN8Ds",
        "colab_type": "text"
      },
      "source": [
        "#Softmax Function and Cross-entropy Loss\n",
        "\n",
        "##$p_{i} = { e^{s_{y_i} - max(s) }\\over\\underset{j}\\sum{ e^ {s_{j} - max(s)}  } }$\n",
        "\n",
        "##$L_{i} = -\\ log( p_{i}) $\n",
        "\n",
        "##$L = {1\\over N}\\underset{i=1}{\\overset{N}\\sum} L_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGUiuMmwf6nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(scores, y_i):\n",
        "    \"\"\"\n",
        "    \n",
        "    scores는 svm_loss와 마찬가지로 점수를 나타내는 (3,)차원 벡터(배열)입니다.\n",
        "    \n",
        "    또한 y_i도 svm_loss와 마찬가지로 정답 라벨을 의미합니다.\n",
        "    \n",
        "    따라서 우리가 구하고자 하는 p_i 는 scores[y_i] - max(scores)를 exponential한 값을\n",
        "    \n",
        "    모든 score의 값에 max(scores)를 뺀 값을 exp로 계산하여 더한 값으로 나누면 나올 것입니다.\n",
        "    \n",
        "    이렇게 구해진 p_i를 return 하면 됩니다.\n",
        "    \n",
        "    exponential 함수는 np.exp()를 사용할 수 있습니다.\n",
        "    \n",
        "    (사실, np.exp함수는 element-wise하기 때문에, softmax를 적용한 배열을 리턴하도록 만들수도 있습니다. 차후 올라갈 코드는 이러한 방식을 사용할 것입니다.)\n",
        "    \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftlRea5LVdc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_loss(scores, y_i):\n",
        "    \"\"\"\n",
        "    \n",
        "    cross_entropy_loss 함수는 간단히 구할 수 있습니다.\n",
        "    \n",
        "    softmax(scores, y_i)에 log를 취한 값의 부호를 바꿔주면 L_i를 구할 수 있습니다.\n",
        "    \n",
        "    이것을 리턴하면 됩니다.\n",
        "    \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqA1F9B6dYid",
        "colab_type": "text"
      },
      "source": [
        "#L2 Regularization\n",
        "\n",
        "###$R(W) = \\underset{k}\\sum\\underset{l}\\sum {W_{kl}}^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzyRBvW3eHDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2_reg(W):\n",
        "    \"\"\"\n",
        "    \n",
        "    W는 (3,4)차원의 행렬이므로 위의 식에서 k는 3, l은 4가 될 것입니다.\n",
        "    \n",
        "    즉, 다음과 같은 형태가 될 것입니다.\n",
        "    \n",
        "    for k in range(3):\n",
        "        for l in range(4):\n",
        "            W[k][l] ...\n",
        "    \n",
        "    L2 regularization 함수는 이러한 W의 각 element 값을 제곱하여 더해준 것이 될 것입니다.\n",
        "    \n",
        "    만일 위 과정을 쉽게 할 수 있으시다면, numpy를 이용하여 for문 없이 regularization term을 계산해 보세요!\n",
        "    \n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjY1gG9ZZv3I",
        "colab_type": "text"
      },
      "source": [
        "# Compute Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V64DeEpSgIsK",
        "colab_type": "code",
        "outputId": "7d645bba-604c-4c78-f336-9ba7bef57baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "np.random.seed(123)\n",
        "weight = np.random.uniform(-2, 2, (3,4))\n",
        "bias = np.random.uniform(-5, 5, (3,))\n",
        "\n",
        "print(weight)\n",
        "print(bias)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.78587674 -0.85544266 -1.09259419  0.20525908]\n",
            " [ 0.87787588 -0.30757416  1.92305679  0.73931895]\n",
            " [-0.07627239 -0.43152993 -0.62728794  0.91619883]]\n",
            "[-0.61427755 -4.40322103 -1.01955745]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CywcznIwgJfg",
        "colab_type": "code",
        "outputId": "fc8edff9-10bd-4690-d5da-426768baaf35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss = 0\n",
        "r = 0.05\n",
        "\n",
        "for i in range(n_train_data):\n",
        "    scores = score_function(train_x[i], weight, bias)\n",
        "    #loss += svm_loss(scores, train_y[i])\n",
        "    loss += cross_entropy_loss(scores, train_y[i])\n",
        "\n",
        "loss /= n_train_data\n",
        "loss += r * l2_reg(weight)\n",
        "\n",
        "print(\"Train loss is {:.3f}\".format(loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train loss is 3.348\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}