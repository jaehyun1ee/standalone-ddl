{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec05_EMIST_Preprocess.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenko99/Standalone_DDL/blob/master/Lec05/Lec05_EMIST_Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-uQwqXGXo4x",
        "colab_type": "text"
      },
      "source": [
        "#EMNIST Data\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1-PLzPJ8X8dh1Gwx-Expdd7geKjW0AuSZ)\n",
        "\n",
        "Data preprocessing 해보기\n",
        "\n",
        "1. 직접 data upload\n",
        "\n",
        "2. data shape 파악\n",
        "\n",
        "3. data visualization\n",
        "\n",
        "4. 사용할 model 구상 - (MLP, CNN 등등...)\n",
        "\n",
        "5. model에 맞게 data process\n",
        "\n",
        "##*Data is from [here](https://www.kaggle.com/crawford/emnist)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gYL5mG3ZRtm",
        "colab_type": "text"
      },
      "source": [
        "###1. Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNXkzUr0TSEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQaYUStrZVNI",
        "colab_type": "text"
      },
      "source": [
        "###2. Read Data and Processing\n",
        "\n",
        "data와 labels를 만들어봅시다!\n",
        "\n",
        "1. csv에서 file을 읽어옵니다. \n",
        "\n",
        "2. 읽은 file을 label과 data로 분리해 줍니다.\n",
        "\n",
        "3. numpy.ndarray type으로 바꾸어줍니다.\n",
        "\n",
        "4. 데이터를 무작위로 섞습니다. (이때, 매번 무작위로 바뀌는 것을 막기위해 seed를 추가합니다.)\n",
        "\n",
        "5. train/validation/test set을 구분해줍니다.\n",
        "\n",
        "Shape는 다음과 같아야 할 것입니다.\n",
        "\n",
        "data = $(88800, 784)$\n",
        "\n",
        "labels = $(88800, )$\n",
        "\n",
        "test_data = $(14800, 784)$\n",
        "\n",
        "test_labels = $(14800, )$\n",
        "\n",
        "(hint : Lec02_NoTrain 을 참조해보세요!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOxuAhnzv7Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3JBBoj7VPie",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('emnist-letters-test.csv', header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu2587WgZZQo",
        "colab_type": "text"
      },
      "source": [
        "###3. Data Visualization\n",
        "\n",
        "아래의 코드는 Lec04_Keras_Tutorial에서 가져온 것입니다.\n",
        "\n",
        "그때 데이터는 MNIST 데이터셋이었습니다.\n",
        "\n",
        "즉, train_data는 $(50000, 28, 28)$ 의 shape를 가지고,\n",
        "\n",
        "train_labels는 $(50000,)$ 의 shape를 가졌습니다.\n",
        "\n",
        "하지만 우리가 가지고 있는 data는 아마 별 다른 조작을 가해주지 않았다면 $(?, 784)$의 shape를 가질 것입니다.\n",
        "\n",
        "따라서 아래 코드를 실행하려면 data를 $(?, 28, 28)$로 바꿔줘야 할 것 같네요!\n",
        "\n",
        "(hint : numpy의 reshape를 이용한다면..? Lec0_Colab&Numpy_Tutorial에 나와있을지도 모르겠네요! )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm_7u-rWZqJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = 0\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "    ax = fig.add_subplot(2, 3, i + 1)\n",
        "    image = hello[idx]\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(\"label = {}\".format(labels[idx]))\n",
        "    idx += 1\n",
        "plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"The shape of image is {}\".format(image.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntrVWy51wjty",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "data = data.reshape(,,).transpose(,,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynAh7-SqZcVC",
        "colab_type": "text"
      },
      "source": [
        "###4. Construct Model\n",
        "\n",
        "keras를 통해 쉽게 할 수 있을 것입니다. loss function은 무엇을 이용해야 할까요?\n",
        "\n",
        "sparse_categorical_crossentropy와 categorical_crossentropy가 어떻게 달랐을까요?\n",
        "\n",
        "Cross entropy loss를 이용하지 않는 다른 방법은 없을까요..?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oaabsx2Zqtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    \"\"\"model construct\"\"\"\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "             loss = \"\"\"what will be the loss function?\"\"\",\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLpShpjEZtgZ",
        "colab_type": "text"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kpIcYRoZwPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = model.fit(train_data, train_labels, epochs=5, batch_size=32, validation_data=(val_data, val_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YtO4ijoZz60",
        "colab_type": "text"
      },
      "source": [
        "### Result Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2IpqWPNZ3Vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
        "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}