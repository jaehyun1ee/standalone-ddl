{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec03_IrisClassification_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenko99/Standalone_DDL/blob/master/Lec03/Lec03_IrisClassification_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a7isqC46O03",
        "colab_type": "text"
      },
      "source": [
        "Open in Colab이 열리지 않으면 [여기](https://drive.google.com/open?id=1psUMkHwv-Aba1O4iPGwuNMxYRseeus6X)를 클릭하세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaQCcEoQ5U4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "iris_data = iris.data\n",
        "iris_labels = iris.target\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(iris_data)\n",
        "df2 = pd.DataFrame(iris_labels)\n",
        "df = pd.concat([df, df2], axis=1, sort=False)\n",
        "df.columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Classes']\n",
        "\n",
        "np.random.seed(7)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "iris_data = df.iloc[:, :4].values\n",
        "iris_labels = df.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHPT_aReAKLs",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Set train_x and train_y for training data\n",
        "\n",
        "Set test_x and test_y for testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFY2GS8Ot2k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_train_data = 135\n",
        "\n",
        "train_x = iris_data[ : n_train_data]\n",
        "train_y = iris_labels[ : n_train_data]\n",
        "test_x = iris_data[n_train_data : ]\n",
        "test_y = iris_labels[n_train_data : ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udSBKbq95J7-",
        "colab_type": "text"
      },
      "source": [
        "# Score Function\n",
        "\n",
        "\n",
        "$ f(X) = WX + B$\n",
        "\n",
        "$W.shape = (3,4)$\n",
        "\n",
        "$X.shape = (4, )$\n",
        "\n",
        "$B.shape = (3, )$\n",
        "\n",
        "$f(X).shape = (3, )$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYhfGsFw26Av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_function(X, weight, bias):\n",
        "    return weight.dot(X) + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JKrUpOhn92t",
        "colab_type": "text"
      },
      "source": [
        "#SVM Loss\n",
        "\n",
        "##$L_{i} = \\underset{j\\neq y_i}\\sum max(0 \\ ,\\ s_j - s_{y_i} + \\triangle )$\n",
        "\n",
        "##$L = {1\\over N}\\underset{i=1}{\\overset{N}\\sum} L_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS_yshtsn9Ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svm_loss(scores, y_i):\n",
        "    loss = np.maximum(0 , scores - scores[y_i] + 1)\n",
        "    loss[y_i] = 0\n",
        "    return np.sum(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4J4Mk9veS9g",
        "colab_type": "text"
      },
      "source": [
        "#Softmax Function and Cross-entropy Loss\n",
        "\n",
        "##$p_{i} = { e^{s_{y_i} - max(s) }\\over\\underset{j}\\sum{ e^ {s_{j} - max(s)}  } }$\n",
        "\n",
        "##$L_{i} = -\\ log( p_{i}) $\n",
        "\n",
        "##$L = {1\\over N}\\underset{i=1}{\\overset{N}\\sum} L_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnvsLzguwiDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(scores):\n",
        "    exps = np.exp(scores - np.max(scores))\n",
        "    return exps / np.sum(exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8g0f_QvoNY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_loss(scores, y_i):\n",
        "    return - np.log(softmax(scores)[y_i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd3xKoggQ-cl",
        "colab_type": "text"
      },
      "source": [
        "#L2 Regularization\n",
        "\n",
        "###$R(W) = \\underset{k}\\sum\\underset{l}\\sum {W_{kl}}^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVc3sh86Q-8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2_reg(W):\n",
        "    return np.sum(np.square(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsrQGbB2_jN1",
        "colab_type": "text"
      },
      "source": [
        "#Numeric Gradient Check\n",
        "\n",
        "##$f'(W) = {f(W + h) - f(W - h) \\over 2h}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWaGFO6RAGJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numeric_grad_check(W, B, loss_fun, h, lmb):\n",
        "    grad_w = np.zeros(W.shape)\n",
        "    grad_b = np.zeros(B.shape)\n",
        "    \n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            w1 = W.copy()\n",
        "            w2 = W.copy()\n",
        "            w1[i][j] += h\n",
        "            w2[i][j] -= h\n",
        "            for k in range(len(train_x)):\n",
        "                grad_w[i][j] += ( loss_fun(score_function(train_x[k], w1, B), train_y[k]) - loss_fun(score_function(train_x[k], w2, B), train_y[k]) ) /  (2*h)\n",
        "    grad_w /= len(train_x)\n",
        "    grad_w += lmb * 2 * W\n",
        "    \n",
        "    for i in range(B.shape[0]):\n",
        "        b1 = B.copy()\n",
        "        b2 = B.copy()\n",
        "        b1[i] += h\n",
        "        b2[i] -= h\n",
        "        for j in range(len(train_x)):\n",
        "            grad_b[i] += ( loss_fun(score_function(train_x[j], W, b1), train_y[j]) - loss_fun(score_function(train_x[j], W, b2), train_y[j]) ) /  (2*h)\n",
        "    grad_b /= n_train_data\n",
        "    \n",
        "    return grad_w, grad_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcJl9C_Jeakc",
        "colab_type": "text"
      },
      "source": [
        "# Train\n",
        "\n",
        "\n",
        "###$Gradient \\ Update\\ (SVM)$\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial w_{y_i} } = - \\underset{j \\ne y_i}\\sum 1(s_j - s_{y_i} + 1 > 0) \\cdot x_{i}^{T} \\\\\n",
        "{\\partial L_{i} \\over \\partial w_{j} } = 1(s_j - s_{y_i} + 1 > 0) \\cdot x_{i}^{T}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial b_{y_i} } = - \\underset{j \\ne y_i}\\sum 1(s_j - s_{y_i} + 1 > 0)\\\\\n",
        "{\\partial L_{i} \\over \\partial b_{j} } = 1(s_j - s_{y_i} + 1 > 0) \n",
        "\\end{cases}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TrGswpc128r",
        "colab_type": "text"
      },
      "source": [
        "###$Gradient \\ Update\\ (Cross-Entropy)$\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial w_{y_i} } = - x_i + p_i \\cdot x_{i}^{T} \\\\\n",
        "{\\partial L_{i} \\over \\partial w_{j} } = p_i \\cdot x_{i}^{T}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial b_{y_i} } = - 1+ p_i  \\\\\n",
        "{\\partial L_{i} \\over \\partial b_{j} } = p_i\n",
        "\\end{cases}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILKHIJi8S-Mk",
        "colab_type": "text"
      },
      "source": [
        "##수정한 부분:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    if loss_fun is svm_loss:\n",
        "        for elem in j:\n",
        "            if scores[elem] - scores[y] + 1 > 0:\n",
        "                grad_w[y, ] -= x\n",
        "                grad_w[elem, ] += x\n",
        "                grad_b[y, ] -= 1\n",
        "                grad_b[elem, ] += 1\n",
        "```\n",
        "\n",
        "위의 수식이 잘 보이지 않으면 colab에서 열어주세요.\n",
        "\n",
        "svm에 대한 수식을 잘못 적어놓기도 했고, $ s_j - s_{y_i} + 1 $을 매 $s_j$ 마다 구해주지 않아서 numreic gradient와 차이가 있었던 것 같습니다.  현재는 모두 수정하였으니, 참고해주세요. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afNwYS6U8dc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_update(weight, bias, loss_fun, i, x, y):\n",
        "    grad_w = np.zeros(weight.shape)\n",
        "    grad_b = np.zeros(bias.shape)\n",
        "    \n",
        "    scores = score_function(x, weight, bias)\n",
        "    loss = loss_fun(scores, y)\n",
        "    \n",
        "    j = [0, 1, 2]\n",
        "    j.remove(y)\n",
        "    if loss_fun is svm_loss:\n",
        "        for elem in j:\n",
        "            if scores[elem] - scores[y] + 1 > 0:\n",
        "                grad_w[y, ] -= x\n",
        "                grad_w[elem, ] += x\n",
        "                grad_b[y, ] -= 1\n",
        "                grad_b[elem, ] += 1\n",
        "    elif loss_fun is cross_entropy_loss:\n",
        "        grad_w += softmax(scores).reshape(3,1) * train_x[i]\n",
        "        grad_w[train_y[i], ] -= train_x[i]\n",
        "        grad_b += softmax(scores)\n",
        "        grad_b[train_y[i], ] -= 1\n",
        "    \n",
        "    return grad_w, grad_b "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRU2GNOD6OjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(weight, bias, lr, lmb, loss_fun):\n",
        "    \n",
        "    train_loss = 0\n",
        "    grad_w = np.zeros(weight.shape)\n",
        "    grad_b = np.zeros(bias.shape)\n",
        "    acc = 0\n",
        "    \n",
        "    for i in range(135):\n",
        "        scores = score_function(train_x[i], weight, bias)\n",
        "        loss = loss_fun(scores, train_y[i])\n",
        "        train_loss += loss\n",
        "        \n",
        "        update_w, update_b = gradient_update(weight, bias, loss_fun, i, train_x[i], train_y[i])\n",
        "        grad_w += update_w\n",
        "        grad_b += update_b\n",
        "        \n",
        "        if train_y[i] == scores.argmax():\n",
        "            acc += 1\n",
        "    \n",
        "    train_loss /= n_train_data\n",
        "    train_loss += lmb * l2_reg(weight)\n",
        "    \n",
        "    grad_w /= n_train_data\n",
        "    grad_w += 2 * lmb * weight\n",
        "    grad_b /= n_train_data\n",
        "    \n",
        "    #grad_w, grad_b = numeric_grad_check(weight, bias, loss_fun, h=0.00001, lmb=lmb)\n",
        "    \n",
        "    weight -= lr * grad_w\n",
        "    bias -= lr * grad_b\n",
        "    \n",
        "    result = {}\n",
        "    result['weight'] = weight\n",
        "    result['accuracy'] = acc / n_train_data\n",
        "    result['train_loss'] = train_loss\n",
        "    result['gradient_w'] = grad_w\n",
        "    result['bias'] = bias\n",
        "    result['gradient_b'] = grad_b\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyf-vZ7OtBo-",
        "colab_type": "text"
      },
      "source": [
        "# Test\n",
        "\n",
        "- calculate accuracy for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpuHtaHc1l5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(weight, bias, loss_fun):\n",
        "    \n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    for i in range(15):\n",
        "        scores = score_function(test_x[i], weight, bias)\n",
        "        \n",
        "        test_loss += loss_fun(scores, test_y[i])\n",
        "        \n",
        "        if test_y[i] == scores.argmax():\n",
        "            acc += 1\n",
        "                \n",
        "    test_loss /= len(test_x)\n",
        "    \n",
        "    result = {}\n",
        "    result['test_loss'] = test_loss\n",
        "    result['accuracy'] = acc / len(test_x)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsrgYJGJlzhF",
        "colab_type": "text"
      },
      "source": [
        "# Experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEbcMuiIK_QC",
        "colab_type": "code",
        "outputId": "9905b70c-105e-4be0-8a48-5c3b29ada2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "np.random.seed(123)\n",
        "Weight = np.random.uniform(-2, 2, (3,4))\n",
        "Bias = np.random.uniform(-5, 5, (3,))\n",
        "\n",
        "grad_wn, grad_bn = numeric_grad_check(Weight, Bias, svm_loss, 0.00001, lmb=0.01)\n",
        "result = train(Weight, Bias, lr=0.05, lmb=0.01, loss_fun=svm_loss)\n",
        "grad_wa = result['gradient_w']\n",
        "grad_ba = result['gradient_b']\n",
        "\n",
        "\n",
        "print(grad_wa[0][0])\n",
        "print(grad_wn[0][0])\n",
        "print(grad_ba[0])\n",
        "print(grad_bn[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46016197929227354\n",
            "0.4601619792923889\n",
            "-0.014814814814814815\n",
            "-0.014814814808168302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRMFiUcnuGH4",
        "colab_type": "code",
        "outputId": "877df1bc-e51e-4817-ad9d-a82cd6a9f9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "np.random.seed(123)\n",
        "weight = np.random.uniform(-2, 2, (3,4))\n",
        "bias = np.random.uniform(-5, 5, (3,))\n",
        "lr = 0.005\n",
        "lmb = 0.001\n",
        "epoch = 1000\n",
        "loss_fun = svm_loss\n",
        "\n",
        "hyperp = {}\n",
        "hyperp['lr'] = lr\n",
        "hyperp['epoch'] = epoch\n",
        "hyperp['lambda'] = lmb\n",
        "hyperp['loss_fun'] = loss_fun\n",
        "\n",
        "result = {}\n",
        "result['train_loss'] = []\n",
        "result['epoch'] = []\n",
        "result['gradient'] = []\n",
        "result['accuracy'] = []\n",
        "result['val_loss'] = []\n",
        "result['val_epoch'] = []\n",
        "\n",
        "for i in range(epoch):\n",
        "    traind = train(weight, bias, lr, lmb, loss_fun)\n",
        "    result['train_loss'].append(traind['train_loss'])\n",
        "    result['epoch'].append(i)\n",
        "    result['gradient'].append(traind['gradient_w'])\n",
        "    result['accuracy'].append(traind['accuracy'])\n",
        "    weight, bias = traind['weight'], traind['bias']\n",
        "    \n",
        "    if i%10 == 0:\n",
        "        val = validation(weight, bias, loss_fun)\n",
        "        result['val_loss'].append(val['test_loss'])\n",
        "        result['val_epoch'].append(i)\n",
        "\n",
        "print(\"Accuaracy for train set is {0:2.2%}\".format(traind['accuracy']))\n",
        "\n",
        "print(\"Accuaracy for validation set is {0:2.2%}\".format(val['accuracy']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuaracy for train set is 87.41%\n",
            "Accuaracy for validation set is 86.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3deMC8N7uSVk",
        "colab_type": "text"
      },
      "source": [
        "# Plot Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8kXV2aJuU26",
        "colab_type": "code",
        "outputId": "0bd53fca-b48e-4d7f-e3f9-41d6f1ef5672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.title('Train loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train_loss')\n",
        "plt.plot(result['epoch'], result['train_loss'])\n",
        "plt.plot(result['val_epoch'], result['val_loss'], color = 'orange')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.title('Gradient')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('gradient')\n",
        "plt.plot(result['epoch'], [result['gradient'][i][0][0] for i in range(epoch)], color = 'red')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"learning rate is {0} and epoch is {1}\".format(hyperp['lr'], hyperp['epoch']))\n",
        "print(\"regularization rate is {0} and loss function is {1}\".format(hyperp['lambda'], hyperp['loss_fun']))\n",
        "print(\"Accuracy For Train Set is {0:2.2%}\".format(result['accuracy'][-1]))\n",
        "print(\"Accuracy For Test Set is {0:2.2%}\".format(val['accuracy']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGUguu+P1ywAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "learning rate is 0.005 and epoch is 1000\n",
            "regularization rate is 0.001 and loss function is <function svm_loss at 0x7f007d937bf8>\n",
            "Accuracy For Train Set is 87.41%\n",
            "Accuracy For Test Set is 86.67%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}