{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec03_IrisClassification_sk.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenko99/Standalone_DDL/blob/master/Lec03/Lec03_IrisClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a7isqC46O03",
        "colab_type": "text"
      },
      "source": [
        "Open in Colab이 열리지 않으면 [여기](https://drive.google.com/open?id=10g-5Ltp42V1UbEsPrFT0WI3EfYpaFZJe)를 클릭하세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaQCcEoQ5U4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "iris_data = iris.data\n",
        "iris_labels = iris.target\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(iris_data)\n",
        "df2 = pd.DataFrame(iris_labels)\n",
        "df = pd.concat([df, df2], axis=1, sort=False)\n",
        "df.columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Classes']\n",
        "\n",
        "np.random.seed(7)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "\n",
        "iris_data = df.iloc[:, :4].values\n",
        "iris_labels = df.iloc[:, 4].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHPT_aReAKLs",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Set train_x and train_y for training data\n",
        "\n",
        "Set test_x and test_y for testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFY2GS8Ot2k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_train_data = 135\n",
        "\n",
        "train_x = iris_data[ : n_train_data]\n",
        "train_y = iris_labels[ : n_train_data]\n",
        "test_x = iris_data[n_train_data : ]\n",
        "test_y = iris_labels[n_train_data : ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udSBKbq95J7-",
        "colab_type": "text"
      },
      "source": [
        "# Score Function\n",
        "\n",
        "\n",
        "$ f(X) = WX + B$\n",
        "\n",
        "$W.shape = (3,4)$\n",
        "\n",
        "$X.shape = (4, )$\n",
        "\n",
        "$B.shape = (3, )$\n",
        "\n",
        "$f(X).shape = (3, )$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYhfGsFw26Av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_function(X, weight, bias):\n",
        "    return weight.dot(X) + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JKrUpOhn92t",
        "colab_type": "text"
      },
      "source": [
        "#SVM Loss\n",
        "\n",
        "##$L_{i} = \\underset{j\\neq y_i}\\sum max(0 \\ ,\\ s_j - s_{y_i} + \\triangle )$\n",
        "\n",
        "##$L = {1\\over N}\\underset{i=1}{\\overset{N}\\sum} L_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS_yshtsn9Ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svm_loss(scores, y_i):\n",
        "    loss = np.maximum(0 , scores - scores[y_i] + 1)\n",
        "    loss[y_i] = 0\n",
        "    return np.sum(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4J4Mk9veS9g",
        "colab_type": "text"
      },
      "source": [
        "#Softmax Function and Cross-entropy Loss\n",
        "\n",
        "##$p_{i} = { e^{s_{y_i} - max(s) }\\over\\underset{j}\\sum{ e^ {s_{j} - max(s)}  } }$\n",
        "\n",
        "##$L_{i} = -\\ log( p_{i}) $\n",
        "\n",
        "##$L = {1\\over N}\\underset{i=1}{\\overset{N}\\sum} L_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnvsLzguwiDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(scores):\n",
        "    exps = np.exp(scores - np.max(scores))\n",
        "    return exps / np.sum(exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8g0f_QvoNY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_loss(scores, y_i):\n",
        "    return - np.log(softmax(scores)[y_i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd3xKoggQ-cl",
        "colab_type": "text"
      },
      "source": [
        "#L2 Regularization\n",
        "\n",
        "###$R(W) = \\underset{k}\\sum\\underset{l}\\sum {W_{kl}}^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVc3sh86Q-8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2_reg(W):\n",
        "    return np.sum(np.square(W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsrQGbB2_jN1",
        "colab_type": "text"
      },
      "source": [
        "#Numeric Gradient Check\n",
        "\n",
        "##$f'(W) = {f(W + h) - f(W - h) \\over 2h}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWaGFO6RAGJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numeric_grad_check(W, B, loss_fun, h, lmb):\n",
        "    grad_w = np.zeros(W.shape)\n",
        "    grad_b = np.zeros(B.shape)\n",
        "    \n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            w1 = W.copy()\n",
        "            w2 = W.copy()\n",
        "            w1[i][j] += h\n",
        "            w2[i][j] -= h\n",
        "            for k in range(len(train_x)):\n",
        "                grad_w[i][j] += ( loss_fun(score_function(train_x[k], w1, B), train_y[k]) - loss_fun(score_function(train_x[k], w2, B), train_y[k]) ) /  (2*h)\n",
        "    grad_w /= len(train_x)\n",
        "    grad_w += lmb * 2 * W\n",
        "    \n",
        "    for i in range(B.shape[0]):\n",
        "        b1 = B.copy()\n",
        "        b2 = B.copy()\n",
        "        b1[i] += h\n",
        "        b2[i] -= h\n",
        "        for j in range(len(train_x)):\n",
        "            grad_b[i] += ( loss_fun(score_function(train_x[j], W, b1), train_y[j]) - loss_fun(score_function(train_x[j], W, b2), train_y[j]) ) /  (2*h)\n",
        "    grad_b /= n_train_data\n",
        "    \n",
        "    return grad_w, grad_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcJl9C_Jeakc",
        "colab_type": "text"
      },
      "source": [
        "# Train\n",
        "\n",
        "\n",
        "###$Gradient \\ Update\\ (SVM)$\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial w_{y_i} } = 1(s_j - s_{y_i} + 1 > 0) \\cdot x_{i}^{T} \\\\\n",
        "{\\partial L_{i} \\over \\partial w_{j} } = \\underset{j \\ne y_i}\\sum 1(s_j - s_{y_i} + 1 > 0) \\cdot x_{i}^{T}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial b_{y_i} } = 1(s_j - s_{y_i} + 1 > 0) \\\\\n",
        "{\\partial L_{i} \\over \\partial b_{j} } = \\underset{j \\ne y_i}\\sum 1(s_j - s_{y_i} + 1 > 0)\n",
        "\\end{cases}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TrGswpc128r",
        "colab_type": "text"
      },
      "source": [
        "###$Gradient \\ Update\\ (Cross-Entropy)$\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial w_{y_i} } = - x_i + p_i \\cdot x_{i}^{T} \\\\\n",
        "{\\partial L_{i} \\over \\partial w_{j} } = p_i \\cdot x_{i}^{T}\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "##$\\begin{cases}\n",
        "{\\partial L_{i} \\over \\partial b_{y_i} } = - 1+ p_i  \\\\\n",
        "{\\partial L_{i} \\over \\partial b_{j} } = p_i\n",
        "\\end{cases}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afNwYS6U8dc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_update(weight, bias, loss_fun, i, x, y):\n",
        "    grad_w = np.zeros(weight.shape)\n",
        "    grad_b = np.zeros(bias.shape)\n",
        "    \n",
        "    scores = score_function(x, weight, bias)\n",
        "    loss = loss_fun(scores, y)\n",
        "    \n",
        "    j = [0, 1, 2]\n",
        "    j.remove(y)\n",
        "    if loss_fun is svm_loss:\n",
        "        if loss > 0:\n",
        "            grad_w[y, ] -= 2 * x\n",
        "            grad_w[j, ] += x\n",
        "            grad_b[y, ] -= 2\n",
        "            grad_b[j, ] += 1\n",
        "    elif loss_fun is cross_entropy_loss:\n",
        "        grad_w += softmax(scores).reshape(3,1) * train_x[i]\n",
        "        grad_w[train_y[i], ] -= train_x[i]\n",
        "        grad_b += softmax(scores)\n",
        "        grad_b[train_y[i], ] -= 1\n",
        "    \n",
        "    return grad_w, grad_b "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRU2GNOD6OjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(weight, bias, lr, lmb, loss_fun):\n",
        "    \n",
        "    train_loss = 0\n",
        "    grad_w = np.zeros(weight.shape)\n",
        "    grad_b = np.zeros(bias.shape)\n",
        "    acc = 0\n",
        "    \n",
        "    for i in range(135):\n",
        "        scores = score_function(train_x[i], weight, bias)\n",
        "        loss = loss_fun(scores, train_y[i])\n",
        "        train_loss += loss\n",
        "        \n",
        "        update_w, update_b = gradient_update(weight, bias, loss_fun, i, train_x[i], train_y[i])\n",
        "        grad_w += update_w\n",
        "        grad_b += update_b\n",
        "        \n",
        "        if train_y[i] == scores.argmax():\n",
        "            acc += 1\n",
        "    \n",
        "    train_loss /= n_train_data\n",
        "    train_loss += lmb * l2_reg(weight)\n",
        "    \n",
        "    grad_w /= n_train_data\n",
        "    grad_w += 2 * lmb * weight\n",
        "    grad_b /= n_train_data\n",
        "    \n",
        "    #grad_w, grad_b = numeric_grad_check(weight, bias, loss_fun, h=0.00001, lmb=lmb)\n",
        "    \n",
        "    weight -= lr * grad_w\n",
        "    bias -= lr * grad_b\n",
        "    \n",
        "    result = {}\n",
        "    result['weight'] = weight\n",
        "    result['accuracy'] = acc / n_train_data\n",
        "    result['train_loss'] = train_loss\n",
        "    result['gradient_w'] = grad_w\n",
        "    result['bias'] = bias\n",
        "    result['gradient_b'] = grad_b\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyf-vZ7OtBo-",
        "colab_type": "text"
      },
      "source": [
        "# Test\n",
        "\n",
        "- calculate accuracy for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpuHtaHc1l5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(weight, bias, loss_fun):\n",
        "    \n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    \n",
        "    for i in range(15):\n",
        "        scores = score_function(test_x[i], weight, bias)\n",
        "        \n",
        "        test_loss += loss_fun(scores, test_y[i])\n",
        "        \n",
        "        if test_y[i] == scores.argmax():\n",
        "            acc += 1\n",
        "                \n",
        "    test_loss /= len(test_x)\n",
        "    \n",
        "    result = {}\n",
        "    result['test_loss'] = test_loss\n",
        "    result['accuracy'] = acc / len(test_x)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsrgYJGJlzhF",
        "colab_type": "text"
      },
      "source": [
        "# Experience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEbcMuiIK_QC",
        "colab_type": "code",
        "outputId": "7a43f972-3b54-402b-84f2-53d374879732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "np.random.seed(123)\n",
        "Weight = np.random.uniform(-2, 2, (3,4))\n",
        "Bias = np.random.uniform(-5, 5, (3,))\n",
        "\n",
        "grad_wn, grad_bn = numeric_grad_check(Weight, Bias, cross_entropy_loss, 0.00001, lmb=0.01)\n",
        "result = train(Weight, Bias, lr=0.05, lmb=0.01, loss_fun=cross_entropy_loss)\n",
        "grad_wa = result['gradient_w']\n",
        "grad_ba = result['gradient_b']\n",
        "\n",
        "\n",
        "print(grad_wa[0][0])\n",
        "print(grad_wn[0][0])\n",
        "print(grad_ba[0])\n",
        "print(grad_bn[0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.6076415664154293\n",
            "-1.607641566378958\n",
            "-0.323256256182845\n",
            "-0.3232562561881207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRMFiUcnuGH4",
        "colab_type": "code",
        "outputId": "90cee34c-1e69-4e0d-f35a-4cc1e909af1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "np.random.seed(123)\n",
        "weight = np.random.uniform(-2, 2, (3,4))\n",
        "bias = np.random.uniform(-5, 5, (3,))\n",
        "lr = 0.005\n",
        "lmb = 0.001\n",
        "epoch = 1000\n",
        "loss_fun = cross_entropy_loss\n",
        "\n",
        "hyperp = {}\n",
        "hyperp['lr'] = lr\n",
        "hyperp['epoch'] = epoch\n",
        "hyperp['lambda'] = lmb\n",
        "hyperp['loss_fun'] = loss_fun\n",
        "\n",
        "result = {}\n",
        "result['train_loss'] = []\n",
        "result['epoch'] = []\n",
        "result['gradient'] = []\n",
        "result['accuracy'] = []\n",
        "result['val_loss'] = []\n",
        "result['val_epoch'] = []\n",
        "\n",
        "for i in range(epoch):\n",
        "    traind = train(weight, bias, lr, lmb, loss_fun)\n",
        "    result['train_loss'].append(traind['train_loss'])\n",
        "    result['epoch'].append(i)\n",
        "    result['gradient'].append(traind['gradient_w'])\n",
        "    result['accuracy'].append(traind['accuracy'])\n",
        "    weight, bias = traind['weight'], traind['bias']\n",
        "    \n",
        "    if i%10 == 0:\n",
        "        val = validation(weight, bias, loss_fun)\n",
        "        result['val_loss'].append(val['test_loss'])\n",
        "        result['val_epoch'].append(i)\n",
        "\n",
        "print(\"Accuaracy for train set is {0:2.2%}\".format(traind['accuracy']))\n",
        "\n",
        "print(\"Accuaracy for validation set is {0:2.2%}\".format(val['accuracy']))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuaracy for train set is 71.11%\n",
            "Accuaracy for validation set is 66.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3deMC8N7uSVk",
        "colab_type": "text"
      },
      "source": [
        "# Plot Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8kXV2aJuU26",
        "colab_type": "code",
        "outputId": "005692c2-9a53-418d-df9a-967838123679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.title('Train loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('train_loss')\n",
        "plt.plot(result['epoch'], result['train_loss'])\n",
        "plt.plot(result['val_epoch'], result['val_loss'], color = 'orange')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.title('Gradient')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('gradient')\n",
        "plt.plot(result['epoch'], [result['gradient'][i][0][0] for i in range(epoch)], color = 'red')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"learning rate is {0} and epoch is {1}\".format(hyperp['lr'], hyperp['epoch']))\n",
        "print(\"regularization rate is {0} and loss function is {1}\".format(hyperp['lambda'], hyperp['loss_fun']))\n",
        "print(\"Accuracy For Train Set is {0:2.2%}\".format(result['accuracy'][-1]))\n",
        "print(\"Accuracy For Test Set is {0:2.2%}\".format(val['accuracy']))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlJygzMysKTlBmZlZU/r/75v9SYWAUlwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "learning rate is 0.005 and epoch is 1000\n",
            "regularization rate is 0.001 and loss function is <function cross_entropy_loss at 0x7ff818af9510>\n",
            "Accuracy For Train Set is 71.11%\n",
            "Accuracy For Test Set is 66.67%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}